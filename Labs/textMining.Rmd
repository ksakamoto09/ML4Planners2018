---
title: "Text Mining"
author: "Kaz Sakamoto"
output: html_document
---
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, cache = TRUE, message = FALSE)
```

## Text Mining Libraries

```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(tidytext)
library(stringr)
library(tidyr)
library(rvest)
library(udpipe)
library(scales)
## here we are going to find the root_file 
root <- rprojroot::find_rstudio_root_file()
## we are going to set the data file path here
dataDir <- file.path(root, 'Data')
```

## Natural Language Processing

Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. - from [wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)

<iframe src="https://giphy.com/embed/PxSFAnuubLkSA" width="480" height="380" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/with-computers-fascination-PxSFAnuubLkSA">via GIPHY</a></p>

In the 1980s I was convinced that computers would soon be able to simulate the basics of what (I hope) you are doing right now: processing sentences and determining their meanings.

To do this, computers would have to master three things. First, enough **syntax** to uniquely identify the sentence; second, enough **semantics** to extract its literal meaning; and third, enough **pragmatics** to infer the intent behind the utterance, and thus discerning what should be done or assumed given that it was uttered. from[Why Are We Still Waiting for Natural Language Processing?](https://www.chronicle.com/blogs/linguafranca/2013/05/09/natural-language-processing/)

## Text Mining

Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).

Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.

A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted. - from [Wikipedia](https://en.wikipedia.org/wiki/Text_mining)

## Tidy Text

Material from [Text Mining with R](https://www.tidytextmining.com/tidytext.html)
is used to create this lab.

We talked a little bit about tidy data structures, where one observation should have its
own row of data, and text data is no different. What is the unit at which we should keep
in a row? well that would be the token, but what is a token? A token is a unit of text such as
a word, and tokenization is the process of taking a chunk of text like a paragraph for instance
and choopping it up to invidual words.

## Text Terms

- **String** text is normally stored as a charater vector.
- **Corpus** an objet that contain raw strings annotated with additional metadata
- **Document-Term Matrix** or sometimes shortened to DTM. It's a sparse matrix 
describing a collection such as a corpus of documents wiht one row for each document and 
one column for each term. Usually word counts, term frequency or inverse document frequency will
make up the values in the DTM.

## Tokens

Let's take a bunch of Jane Jacobs quotes, which are typical bits of text that you might encounter.

```{r}
jacobsQuotes <- c("Cities have the capability of providing something for everybody, only because, and only when, they are created by everybody.",
"A city street equipped to handle strangers, and to make a safety asset, in itself, our of the presence of strangers, as the streets of successful city neighborhoods always do, must have three main qualities",
"First, there must be a clear demarcation between what is public space and what is private space. Public and private spaces cannot ooze into each other as they do typically in suburban settings or in projects.",
"Second, there must be eyes upon the street, eyes belonging to those we might call the natural proprietors of the street. The buildings on a street equipped to handle strangers and to insure the safety of both residents and strangers, must be oriented to the street. They cannot turn their backs or blank sides on it and leave it blind.",
"And third, the sidewalk must have users on it fairly continuously, both to add to the number of effective eyes on the street and to induce the people in buildings along the street to watch the sidewalks in sufficient numbers. Nobody enjoys sitting on a stoop or looking out a window at an empty street. Almost nobody does such a thing. Large numbers of people entertain themselves, off and on, by watching street activity."
)
```

Let's change the quotes and create a tibble(modern data frame).
```{r}
text_df <- tibble(quote = 1:length(jacobsQuotes), text = jacobsQuotes)

text_df
```

### Unnest Tokens

Now we need to take each quote and break out each word from each row. This process is called tokenization and will create
a tidy data structure for analysis. The two arguments are column names, we have the output name `word` and the input `text`. 
You'll see that we still have where each word came from in the `quote` column. By convenience puntuation also has been
stripped and tokens are convereted to lowercase. 
```{r}
text_df %>% 
    unnest_tokens(word, text)
```

Here is a workflow of a simple text mining operation with unnesting the tokens feeding
into the tidyverse (dplyr) methods of data manipulation.
![](https://www.tidytextmining.com/images/tidyflow-ch-1.png)

```{r}
text_df %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words)

text_df %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words) %>% 
    count(word, sort = TRUE)

```
### Citylab Example

Here is some code to scrape the top 5 stories from [citylab](https://www.citylab.com).

```{r, eval=FALSE}

homePage <- "https://www.citylab.com/"

topStories <- read_html(homePage) %>% 
    html_nodes(css = "a.c-most-popular__link") %>% 
    html_attr(name = "href")

scrapeArticle <- function(url){
    title <- read_html(url) %>% 
        html_nodes("article h1") %>% 
        html_text()
    
    
    text <- read_html(url) %>% 
        html_nodes("article") %>% 
        html_nodes(".s-article__section p") %>% 
        html_text()
    
    tibble(title, text, section = 1:length(text))
}


textDF <- topStories %>% 
    as.list() %>% purrr::map_df(~scrapeArticle(.x))

textDF %>% unnest_tokens(word, text)
```

I've already scraped some data so lets read in data from march 18th.
```{r}
textDF <- read_csv(file.path(dataDir, "cityLab3_18_19.csv"))
```

again we're going to want to unnest the tokens and this time we're going to remove stop_words.
stop words are commonlyu used words like "the", "a", "an", "in", etc...

```{r}
tidy_articles <- textDF %>% 
    unnest_tokens(word, text) %>%
    anti_join(stop_words)

tidy_articles %>% count(word, sort = TRUE)
```

Let's take a look at some graphs of the most common words in our articles
```{r}
library(ggplot2)

tidy_articles %>%
  count(word, sort = TRUE) %>%
  filter(n > 15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
    theme_minimal()
```

### Word Frequencies

Word frequencies are a common metric used in text mining. Here we can take the our articles
and calculate the porportion of the words in the articles. We could also do another analysis
where we group by authors and see how different writers use words proportionately.

```{r}
frequency <- tidy_articles %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>% # take out only words
  count(title, word) %>% # count the title word pairings
  group_by(title) %>% # group by articles
  mutate(proportion = n / sum(n)) %>%  # create the proportion by article
  select(-n) %>% # remove count
  spread(title, proportion) %>% # we're going to spread by title
  gather(title, proportion, 3:6) # we're going to gather by titles except the 2nd column
```

Now we can compare word frequencies agains our first article. Words that are similar
gre going lie along the dotted line. The words closest to the y axis is particular to
the Hudson Yards article, while the words closest to the x axis are particular to the
corresponding other articles.

```{r}

# expect a warning about rows with missing values being removed
frequency %>% ggplot(aes(x = proportion, 
                         y = `Inside Hudson Yards, Manhattan’s Opulent New Mini-City`)) +
    geom_abline(color = "gray40", lty = 2) +
    geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
    scale_x_log10(labels = percent_format()) +
    scale_y_log10(labels = percent_format()) +
    scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
    facet_wrap(~title, ncol = 2) +
    theme(legend.position="none") + 
    theme_minimal() +
    labs(y = "Inside Hudson Yards, Manhattan’s Opulent New Mini-City", x = NULL)

```

We can also check the correlations of the articles with their words. The Durham Light Rail
article is the most correlated with the Hudson Yards article with a correalation of .74 while
interestingly enough has the lowest correlation with the Unpacking the Power of Priviledged Neighborhoods.

```{r}
cor.test(data = frequency[frequency$title == "The Affordable Home Crisis Continues, But Bold New Plans May Help",],
         ~ proportion + `Inside Hudson Yards, Manhattan’s Opulent New Mini-City`)

cor.test(data = frequency[frequency$title == "Thanks to Duke, Durham's Light Rail Dream Is All But Dead",],
         ~ proportion + `Inside Hudson Yards, Manhattan’s Opulent New Mini-City`)

cor.test(data = frequency[frequency$title == "The Case for Rooms",],
         ~ proportion + `Inside Hudson Yards, Manhattan’s Opulent New Mini-City`)

cor.test(data = frequency[frequency$title == "Unpacking the Power of Privileged Neighborhoods",],
         ~ proportion + `Inside Hudson Yards, Manhattan’s Opulent New Mini-City`)
```


## Part of Speech

So far we haven't really been taking into account what kind of words we have been dealing with.
Let's use the `udpipe` [library](https://bnosac.github.io/udpipe/en/index.html).
We'll use this new package to work with parts of speech. These are already pretrained models and will
help up categorize parts of speech, they also come in a myriad of langauges.

```{r}
# run this next line once
#model <-udpipe_download_model(language = "english-ewt") 
udmodel_english <- udpipe_load_model(file = file.path(dataDir,"english-ewt-ud-2.3-181115.udpipe"))
psDF <- udpipe::udpipe_annotate(udmodel_english, textDF$text) %>% data.frame()
```

Let's take a look at what the `psDF` data frame looks like. 
You'll see that we have the `token` information, as well as `upos` which is the
parts of speech. So adjectives, nouns, verbs, etc...

```{r}
glimpse(psDF)
```

Here we can check out counts of parts of speech in our text.

```{r}

txt_freq(psDF$upos) %>% 
    ggplot(aes(x = reorder(key, freq), y = freq)) + 
    geom_col(fill = "green") + coord_flip() + 
    ylab("Count") +
    xlab("Part of Speech")+
    theme_minimal()
```
We will now subset Nouns.

```{r}
nouns <- psDF %>% filter(upos %in% c("NOUN")) %>%pull(token) %>% txt_freq()
nouns %>% 
    slice(1:20) %>% 
    ggplot(aes(x = reorder(key, freq), y = freq))+ 
    geom_col(fill = "purple") + coord_flip() + 
    ylab("Count") +
    xlab("Nouns")+
    theme_minimal()
```

We will now subet Verbs
```{r}
verbs <- psDF %>% filter(upos %in% c("VERB")) %>%pull(token) %>% txt_freq()
verbs %>% 
    slice(1:20) %>% 
    ggplot(aes(x = reorder(key, freq), y = freq))+ 
    geom_col(fill ="navy") + coord_flip() + 
    ylab("Verbs") +
    xlab("Frequency")+
    theme_minimal()
```
and finally Adjectives
```{r}
adjectives <- psDF %>% filter(upos %in% c("ADJ")) %>%pull(token) %>% txt_freq()
adjectives %>% 
    slice(1:20) %>% 
    ggplot(aes(x = reorder(key, freq), y = freq))+ 
    geom_col(fill = "dark green") + coord_flip() + 
    ylab("Adjectives") +
    xlab("Frequency")+
    theme_minimal()
```

### Automated Keyword Extraction

RAKE is one of the most popular (unsupervised) algorithms for extracting keywords in Information retrieval. RAKE short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurrence with other words in the text. [Easy Text Analysis](https://towardsdatascience.com/easy-text-analysis-on-abc-news-headlines-b434e6e3b5b8)
```{r}
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id", 
                       relevant = x$upos %in% c("NOUN", "ADJ"))
stats %>%
    slice(1:20) %>% 
    ggplot(aes(x = reorder(keyword, freq), y = freq))+ 
    geom_col() + coord_flip() + 
    ylab("Adjectives") +
    xlab("Frequency")+
    theme_minimal()
```

Here will simple noun phrases to collect keywords. We'll need to convert our `upos`
column to one letter tags and use some regular expressions to match our keyword
combos. 
```{r}
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos") # convert parts of speech tags to one letter tags
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), #Here are the combinations of keywords we want to collect
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE) %>% 
    subset(ngram > 1 & freq > 3)
stats %>%
    slice(1:20) %>% 
    ggplot(aes(x = reorder(keyword, freq), y = freq))+ 
    geom_col(fill = "blue") + coord_flip() + 
    ylab("Keywords") +
    xlab("Frequency")+
    theme_minimal() + 
    ggtitle("Top Noun Phrases")
```


## Sentiment Analysis

Sentiment Analysis is also referred to as opinion mining and useful for understanding
the emotional nature of text. At the simplest level, positivity and negativity in the 
text can be gleaned. There are more subtle emotions that can also be gained such as
sadness, trust, and fear.

### Sentiments

There are three main lexicons used for sentiment analysis

- `AFINN`
- `bing`
- `nrc`

These lexicons are based on single words(unigrams) and either have positive or negative
scores, or emotions. 

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

note: that these lexicons were created using crowdsourcing or select authors and validated
with an assortment of text sources. Also not all English words are captured in the lexicons
since there are many neutral words. 

```{r}
tidy_articles %>% 
    filter(title == "Unpacking the Power of Privileged Neighborhoods") %>% 
    inner_join(get_sentiments("nrc")) %>% 
    count(sentiment, sort = TRUE)
```


```{r}
articleSentiments <- tidy_articles %>% 
    inner_join(get_sentiments("bing")) %>% 
    count(title, index = section, sentiment) %>% 
    spread(sentiment, n, fill = 0) %>% 
    mutate(sentiment = positive-negative)

```

```{r}
articleSentiments %>% ggplot(aes(index, sentiment, fill = title)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~title, ncol = 2, scales = "free_x")
```

```{r}
bing_word_counts <- tidy_articles %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
    group_by(sentiment) %>%
    top_n(10) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to sentiment",
         x = NULL) +
    coord_flip() + 
    theme_minimal()

```

### Custom stop words

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

### Beyond single words

```{r}
tibble(text = jacobsQuotes) %>% unnest_tokens(sentence, text, token = "sentences")
```

```{r}
textDF %>% filter(title == "Unpacking the Power of Privileged Neighborhoods") %>% 
    sentimentr::get_sentences(text) %>% sentimentr::sentiment() %>% head()
```

```{r}
out <- textDF %>% mutate(title = as.factor(title),
                         section = as.factor(section)) %>% 
     sentimentr::get_sentences() %>% sentimentr::sentiment_by("title")
```

```{r}
plot(out)
```

```{r}
out %>% sentimentr::uncombine() %>% group_by(title,section) %>% 
    summarise(sentiment = sum(sentiment)) %>% ungroup() %>% 
ggplot(aes(x = as.numeric(section), y = sentiment, color = as.character(title))) + geom_line(show.legend = FALSE) + 
    facet_wrap(~title) + theme_minimal()
```


```{r}
 textDF %>% mutate(title = as.factor(title),
                         section = as.factor(section)) %>% 
     sentimentr::get_sentences() %>% sentimentr::sentiment_by("title") %>% 
    sentimentr::highlight()
```

## Word and Document Frequency

One measure of determining the importance of a word is the term-frequency(tf) which is
how frequently does this word show up in the text? Another approach is the inverse document 
frequency(idf) which decreases the weight for commonly used words and increases the weight for words 
that don't show up a lot. By combining these things, we get the tf-idf, which is the frequency
of a term adjusted for how rare it is.

$$idf(term) = log(\frac{n_{documents}}{1+ n_\text{documents containing term}})$$ (using + 1 to avoid dividing by 0)

```{r}
article_words <- textDF %>%
    unnest_tokens(word, text) %>% 
    count(title, word, sort = TRUE)
total_words <- article_words %>% 
    group_by(title) %>% 
    summarize(total = sum(n))

article_words <- left_join(article_words, total_words)
article_words
```
There are very long tails to the right for these articles (those extremely common words!). These plots exhibit similar distributions for all the articles, with many words that occur rarely and fewer words that occur frequently.
```{r}
ggplot(article_words, aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE, binwidth = 0.001) +
  facet_wrap(~title, ncol = 2, scales = "free_y")
```

### Zipf's Law

Distributions like these are typical in language. In fact, those types of long-tailed distributions are so common in any given corpus of natural language (like a book, or a lot of text from a website, or spoken words) that the relationship between the frequency that a word is used and its rank has been the subject of study; a classic version of this relationship is called Zipf’s law, after George Zipf, a 20th century American linguist.

The rank of the word times its probability(frequency) is approximately a constant

```{r}
freq_by_rank <- article_words %>% 
  group_by(title) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank
```

Here we transform our axes by changing them to logs. The articles
all seem to have similar slopes, but there seems to be a section btween 
rank 10 and 100 where we can fit a nice linear relationship.
```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = title)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) + 
  scale_x_log10() +
  scale_y_log10() + 
    theme_minimal() +
    theme(legend.position="bottom")
```


```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 100,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```

Zipf's law is :
$$\text{frequency} \propto \frac{1}{\text{rank}}$$
our slope is -0.85 which is close to -1.

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = title)) + 
  geom_abline(intercept = -1.0546, slope = -0.86, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
    theme_minimal()
```


### TF IDF

The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the top five citylab articles as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. Let’s do that now. (common locally, but rare globally)
```{r}
article_words <- article_words  %>%
  bind_tf_idf(word, title, n) %>% 
    select(-total) %>% 
    arrange(desc(tf_idf))
```

```{r}
article_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(title) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~title, ncol = 2, scales = "free") +
  coord_flip() +
    theme_minimal()
```

## Multiple Words

single words are fine, but the relationship between words are also very important.
We have alredy seen words such as Hudson Yards come up in our articles.

```{r}
article_bigrams <- textDF %>% 
    unnest_tokens(bigram, text, token = "ngrams", n = 2)
article_bigrams
```

```{r}
article_bigrams%>%
  count(bigram, sort = TRUE)
```

```{r}
bigrams_separated <- article_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(title, bigram) %>%
  bind_tf_idf(bigram, title, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

```{r}
bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(title) %>% 
  top_n(n = 6) %>% 
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~title, ncol = 2, scales = "free") +
  coord_flip() +
    theme_minimal()
```

```{r}
not_words <- bigrams_separated %>%
  filter(word1 == "without") %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()
not_words
```

```{r}
library(igraph)
bigram_graph <- bigram_counts %>%
  filter(n > 3) %>%
  graph_from_data_frame()

bigram_graph
```


```{r}
library(ggraph)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
    theme_minimal()
```

## Topic Modelling

